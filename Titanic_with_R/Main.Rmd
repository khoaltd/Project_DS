---
title: "Begining with Titanic"
author: Le Tan Dang Khoa
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---
# Introduction
This project involves the analysis and the prediction of the people who were more likely to survive on the British liner named Titanic, that sank in the North Atlantic Ocean in the early morning of 15 April 1912 after colliding with an iceberg during her maiden voyage from Southampton, UK, to New York City, US.. Titanic only carried enough lifeboats for 1,178 people, slightly more than half of the number of the people on board, and one-third her total capacity.

Reading the data in order to be able to complete the further development.

```{r import lib, include=FALSE}
# Load all the packages required for the analysis
library(dplyr) # Data Manipulation
library(Amelia) # Missing Data: Missings Map
library(ggplot2) # Visualization
library(scales) # Visualization
library(caTools) # Prediction: Splitting Data
library(car) # Prediction: Checking Multicollinearity
library(ROCR) # Prediction: ROC Curve
library(e1071) # Prediction: SVM, Naive Bayes, Parameter Tuning
library(rpart) # Prediction: Decision Tree
library(rpart.plot) # Prediction: Decision Tree
library(randomForest) # Prediction: Random Forest
library(caret) # Prediction: k-Fold Cross Validation
```
With this titanic dataset, I explore five classification algorithms: Logistic Regression, Support Vector Machines (both linear and non-linear), Decision Tree, Random Forest, and Naive Bayes.

## Input Data
```{r read data, include=FALSE}
titanic_train <-read.csv('input/train.csv', stringsAsFactors = FALSE)
titanic_test <- read.csv('input/test.csv', stringsAsFactors = FALSE)
```
```{r combine data}
# Combining data
titanic <- bind_rows(titanic_train, titanic_test)

# Checking the structure of the data
str(titanic)
```
## Handling Missing Data
### Checking Missing Data
```{r sumarry colums}
# Checking missing values (missing values or empty values)
colSums(is.na(titanic)|titanic=='')
```
Cabin has the most number of missing values, 1014 values. Age has 263 missing values while Embarked and Fare have two and one missing values, respectively.  

## Missing Fare Data Imputation
```{r missing Fare}
# Extract the row which contains the missing Fare
filter(titanic, is.na(Fare)==TRUE|Fare=='')
```
This male was from the third class and had embarked from Southampton port. Let’s look at the distribution of third class passengers embarked from Southampton port.
```{r plot Fare}
ggplot(filter(titanic, Pclass==3 & Embarked=="S"), aes(Fare)) +                       
  geom_density(fill="blue", alpha=0.5) +
  geom_vline(aes(xintercept=median(Fare, na.rm=T)), colour='darkblue', linetype='dashed', size=2) +
  geom_vline(aes(xintercept=mean(Fare, na.rm=T)), colour='red', linetype='dashed', size=2) +
  ggtitle("Fare distribution of third class passengers \n embarked from Southampton port") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```
The mean and median fares are not close. The proportion of passengers with fares around median is very high. On the other hand, it is not that high around mean. So, I believe it is not a good idea to impute the missing fare by the mean of all fares. I would rather impute the missing fare by the median fare of third class passengers embarked from Southampton port.
```{r modify Fare data}
# Impute the missing Fare value by the median fare of third class passengers embarked from Southampton port
titanic$Fare[is.na(titanic$Fare)==TRUE] = median(filter(titanic, Pclass==3 & Embarked=="S")$Fare, na.rm=TRUE)

# Checking missing values
colSums(is.na(titanic)|titanic=='')
```
The missing fare has been replaced.  

## Missing Embarked Data Imputation
We have noticed that there are two missing values for the Embarked feature.
```{r missing Embarked }
# Extract the rows which contain the missing Embarked values
filter(titanic, is.na(Embarked)==TRUE|Embarked=='')
```
Both were females from the first class with $80 fare and had stayed at the same cabin B28. There is a high chance that both embarked from the same port. Let’s look at the frequency of ports of embarkation of first class passengers.
```{r overview Embarked}
# Frequency of ports of embarkation of first class passengers
table(filter(titanic, Pclass==1)$Embarked)
```
The Southampton port is the most frequent port of embarkation with 177 ports and it is followed by the Cherbourg port with 141. Wait! Yet, we cannot decide to impute two missing values by the most frequent port of embarkation which is the Southampton port.
```{r }
ggplot(filter(titanic, is.na(Embarked)==FALSE & Embarked!='' & Pclass==1), 
       aes(Embarked, Fare)) +     
  geom_boxplot(aes(colour = Embarked)) +
  geom_hline(aes(yintercept=80), colour='red', linetype='dashed', size=2) +
  ggtitle("Fare distribution of first class passengers") +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```
The box plot depicts the median fare for Cherbourg port passengers and $80 fare paid by two embarkement-deficient passengers almost concide. Thus, I am going to impute the missing Embarked values by the Cherbourg port.
```{r}
# Impute the missing Embarked values by the Cherbourg port
titanic$Embarked[titanic$Embarked==""] = "C"

# Checking missing values
colSums(is.na(titanic)|titanic=='')
```

## Missing Age Data Imputation
```{r}
ggplot(titanic,aes(Pclass,Age)) +                                                  
  geom_boxplot(aes(fill=factor(Pclass)),alpha=0.5) +
  ggtitle("Age distribution based on Pclass")
```
It can be clearly seen the median age among classes is not similar (virtually certain, average age among classes is not similar as well). Infact, the passengers in the higher classes tend to be older. Rather than just imputing missing age values by the overall average for age, I will use average age values of each class to impute missing age values based on Pclass.
```{r}
# Imputation of Age based on Pclass
impute.age <- function(age,class){
  vector <- age
  for (i in 1:length(age)){
    if (is.na(age[i])){
      if (class[i] == 1){
        vector[i] <- round(mean(filter(titanic,Pclass==1)$Age, na.rm=TRUE),0)
      }else if (class[i] == 2){
        vector[i] <- round(mean(filter(titanic,Pclass==2)$Age, na.rm=TRUE),0)
      }else{
        vector[i] <- round(mean(filter(titanic,Pclass==3)$Age, na.rm=TRUE),0)
      }
    }else{
      vector[i]<-age[i]
    }
  }
  return(vector)
}
imputed.age <- impute.age(titanic$Age,titanic$Pclass)
titanic$Age <- imputed.age
```
Let’s check if the above imputation method worked.
```{r}
# Checking missing values
colSums(is.na(titanic)|titanic=='')
```
It worked. Now we are left with only Cabin missing values. However, due to the high number of missing values of Cabin feature, I keep the Cabin feature as it is and stop here.  

# Feature Engineering
## Passenger Title
Since the title of the passengers is contained within the passenger name feature, let’s do some feature engineering to create a new feature with passenger titles.
```{r}
head(titanic$Name)
```
```{r}
# Grab passenger title from passenger name
titanic$Title <- gsub("^.*, (.*?)\\..*$", "\\1", titanic$Name)

# Frequency of each title by sex
table(titanic$Sex, titanic$Title)
```
```{r}
# First, I reassign few categories 
titanic$Title[titanic$Title == 'Mlle' | titanic$Title == 'Ms'] <- 'Miss' 
titanic$Title[titanic$Title == 'Mme']  <- 'Mrs' 

# Then, I create a new category with low frequency of titles
Other <- c('Dona', 'Dr', 'Lady', 'the Countess','Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir')
titanic$Title[titanic$Title %in% Other]  <- 'Other'

# Let's see if it worked
table(titanic$Sex, titanic$Title)
```
The title is down to five categories. We will do exploratory analysis based on title in the next section.

## Family Size

```{r}
FamilySize <- titanic$SibSp + titanic$Parch + 1

table(FamilySize)
```
There are nine family sizes: 1 to 8 and 11. As this is too many categories, let’s collapse some categories as follows.
```{r}
# Create a family size feature with three categories
titanic$FamilySize <- sapply(1:nrow(titanic), function(x) 
                          ifelse(FamilySize[x]==1, "Single", 
                          ifelse(FamilySize[x]>4, "Large", "Small")))

table(titanic$FamilySize)
```
In the next section, we will do some exploratory anaysis based on family size.

# Exploratory Data Analysis
##  Encoding the categorical features as factors
```{r}
titanic$Survived = factor(titanic$Survived)
titanic$Pclass = factor(titanic$Pclass)
titanic$Sex = factor(titanic$Sex)
titanic$Embarked = factor(titanic$Embarked)
titanic$Title = factor(titanic$Title)
titanic$FamilySize = factor(titanic$FamilySize, levels=c("Single","Small","Large"))

#Checking the structure of the data
str(titanic)
```

## Exploratory Data Analysis on Pclass, Sex and Age
```{r}
ggplot(filter(titanic, is.na(Survived)==FALSE), aes(Pclass, fill=Survived)) + 
  geom_bar(aes(y = (..count..)/sum(..count..)), alpha=0.9, position="dodge") +
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  scale_y_continuous(labels=percent, breaks=seq(0,0.6,0.05)) +
  ylab("Percentage") + 
  ggtitle("Survival Rate based on Pclass") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```
Of course as expected, the wealthier passengers in the first class had a higher survival rate, roughly 15%, than the second class and third class passengers.

Let’s continue on by visualising data of some of the features.
```{r}
ggplot(filter(titanic, is.na(Survived)==FALSE), aes(Sex, fill=Survived)) + 
  geom_bar(aes(y = (..count..)/sum(..count..)), alpha=0.9) +
  facet_wrap(~Pclass) + 
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  scale_y_continuous(labels=percent, breaks=seq(0,0.4,0.05)) +
  ylab("Percentage") + 
  ggtitle("Survival Rate based on Pclass and Sex") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
It can be seen that females had a higher survival rate than males in each class. This makes sense due to women and children first policy.
```{r}
ggplot(filter(titanic, is.na(Survived)==FALSE), aes(Pclass, Age)) + 
  geom_violin(aes(fill=Survived), alpha=0.9) +
  facet_wrap(~Survived) + 
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  ggtitle("Survival Rate based on Pclass and Age") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```
Overall, the passengers in the higher classes tend to be older disregard to whether they survived or not.

# Exploratory Data Analysis on Title and FamilySize
```{r}
mosaicplot(~ Title + Survived, data=titanic, main='Survival Rate based on Title', shade=TRUE)
```
Generally, male “Mr” passengers had the poorest survival rate.
```{r}
ggplot(filter(titanic, is.na(Survived)==FALSE), aes(Title)) + 
  geom_bar(aes(fill=Survived), alpha=0.9, position="fill") +
  facet_wrap(~Pclass) + 
  scale_fill_brewer(palette="Set1") +
  scale_y_continuous(labels=percent, breaks=seq(0,1,0.1)) +
  ylab("Percentage") + 
  ggtitle("Survival Rate based on Pclass and Title") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
The same information can be depicted from the above graph - the male “Mr” passengers had the lowest survival rate amongst all the classes.
```{r}
mosaicplot(~ FamilySize + Survived, data=titanic, main='Survival Rate based on FamilySize', shade=TRUE)
```
Large families had the worst survival rate than singletons and small families.
```{r}
ggplot(filter(titanic, is.na(Survived)==FALSE), aes(Title)) + 
  geom_bar(aes(fill=Survived), alpha=0.9, position="fill") +
  facet_wrap(~FamilySize) + 
  scale_fill_brewer(palette="Set1") +
  scale_y_continuous(labels=percent, breaks=seq(0,1,0.1)) +
  ylab("Percentage") + 
  ggtitle("Survival Rate based on FamilySize and Title") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
The filled bar chart illustrates that we can preserve our rule: large families had the worst survival rate than singletons and small families. Infact, each member of the large families - Master, Miss, Mr and Mrs - suffered the lowest survival rate than their counterparts in other types of families.

## Exploratory Data Analysis on Fare and Embarked
```{r}
ggplot(filter(titanic, is.na(Survived)==FALSE), aes(Embarked, Fare)) + 
  geom_boxplot(aes(fill=Survived), alpha=0.9) +
  facet_wrap(~Survived) + 
  scale_fill_manual(values=c("#56B4E9", "#CC79A7")) +
  ggtitle("Survival Rate based on Embarked and Fare") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```
Interestingly, there is a substantial variation of fares in the survived category, especially from Cherbourg and Southampton ports.

Visual analysis of data concludes:

* the wealthier passengers in the first class had a higher survival rate;
* females had a higher survival rate than males in each class;
* male “Mr” passengers had the lowest survival rate amongst all the classes; and
large families had the worst survival rate than singletons and small families.

After rectifying the missing values and exploring data visually, finally, we are ready to predict whether or not each passenger in the test set survived the sinking of the Titanic. I identify the following features may contribute to the prediction of the survival and include them in the classification algorithms: Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, Title and FamilySize.

* I ignore the feature Name as I have created a new feature Title from it and I believe the title has more predictive power than just name.

* I ignore the feature Ticket as I believe it does not preserve any predictive power on survival.

* I ignore the feature Cabin since it has many missing values.

# Prediction
## Splitting the dataset into the Training set and Test set
After rectifying the missing values and encoding the categorical features as factors, now we are good to split the dataset into the training and test sets.
```{r}

```

```{r}
# Splitting the dataset into the Training set and Test set
train_original <- titanic[1:891, c("Survived","Pclass","Sex","Age","SibSp","Parch","Fare","Embarked","Title","FamilySize")]
test_original <- titanic[892:1309, c("Pclass","Sex","Age","SibSp","Parch","Fare","Embarked","Title","FamilySize")]
```

## Splitting the training set into the Training set and Validation set
Now I split the training set into the training set (80% of training data) and validation set (20% of training data) for the evaluation purposes of the fitted models.
```{r}
# Splitting the Training set into the Training set and Validation set
set.seed(789)
split = sample.split(train_original$Survived, SplitRatio = 0.8)
train = subset(train_original, split == TRUE)
test = subset(train_original, split == FALSE)
```

## Logistic Regression
```{r}
# Show the correlation of numeric features
cor(train[,unlist(lapply(train,is.numeric))])
```
In statistics, two variables are strongly correlated if the correlation coefficient is either greater than 0.75 (some say 0.70 and some even say 0.8) or less than -0.75. Having a glance at the correlation matrix, none of the numeric features are strongly correlated. Hence, the **Multicollinearity** (a given feature in the model can be approximated by a linear combination of the other features in the model) does not exist among numeric features.
```{r}
# Show the p-value of Chi Square tests
ps = chisq.test(train$Pclass, train$Sex)$p.value
pe = chisq.test(train$Pclass, train$Embarked)$p.value
pt = chisq.test(train$Pclass, train$Title)$p.value
pf = chisq.test(train$Pclass, train$FamilySize)$p.value
se = chisq.test(train$Sex, train$Embarked)$p.value
st = chisq.test(train$Sex, train$Title)$p.value
sf = chisq.test(train$Sex, train$FamilySize)$p.value
et = chisq.test(train$Embarked, train$Title)$p.value
ef = chisq.test(train$Embarked, train$FamilySize)$p.value
tf = chisq.test(train$Title, train$FamilySize)$p.value
cormatrix = matrix(c(0, ps, pe, pt, pf,
                     ps, 0, se, st, sf,
                     pe, se, 0, et, ef,
                     pt, st, et, 0, tf,
                     pf, sf, ef, tf, 0), 
                   5, 5, byrow = TRUE)
row.names(cormatrix) = colnames(cormatrix) = c("Pclass", "Sex", "Embarked", "Title", "FamilySize")
cormatrix
```
I use Chi Square test to test the independence of factors/categorical features. Since all the p-values < 0.05, we reject each Ho:Two factors are independent at 5% significance level and indeed at any reasonable level of significance. This violates the independence assumption of features and can be confirmed that multicollinearity does exist among factors. I will deal with this issue down the road and now go ahead and fit the logistic regression model.
```{r}
# Fitting Logistic Regression to the Training set
classifier = glm(Survived ~ ., family = binomial(link='logit'), data = train)

# Choosing the best model by AIC in a Stepwise Algorithm
# The step() function iteratively removes insignificant predictor variables from the model.
classifier <- step(classifier)
```
```{r}
summary(classifier)
```
The factor Sex is not statistically significant at any reasonable level of significance (p-value = 0.976960 > 0.05 or 0.01 or even 0.1); however, it is still in the best model. Furthermore, notice the standard error of **Sexmale**, **TitleMiss** and **TitleMrs** are very large. This has something to do with multicollinearity. As an effect of multicollinearity, the standard error (hence, the variance) of model coefficients for Sexmale, TitleMiss and TitleMrs became very large.

```{r}
vif(classifier)
```
vif function delivers very high Generalized Variable Inflation Factor (GVIF) for factors Sex and Title. This confirmes the multicollinearity between factors **Sex** and **Title**.
We want to make our model robust. I omit the factor **Sex** from the logistic regression model because it exhibits a high degree of multicollinearity.
```{r}
# Fitting Logistic Regression to the Training set again without the factor Sex 
classifier <- glm(Survived ~ . -Sex, family = binomial(link='logit'), data = train)


# Choosing the best model by AIC in a Stepwise Algorithm
# The step() function iteratively removes insignificant features from the model.
classifier <- step(classifier)
```
```{r}
summary(classifier)
```
```{r}
vif(classifier)
```

```{r}
durbinWatsonTest(classifier)
```
The model looks good. The standard errors are in a reasonable range. GVIF values ​​are all less than 5. Furthermore, since Durbin-Watson test results with D-W Statistic 1.96 and p-value > 0.05, we do not reject Ho:Residuals are not autocorrelated. Hence, we can consequently there is sufficient evidence to say residuals are not autocorrelated. Hooray! The assumptions are checked and they are passed.

According to the best model, the features **Pclass**, **Age**, **Title** and **FamilySize** significantly contribute to the model in predicting survival. We will see how well the model predicts on new data in the validation set.
```{r}
# Predicting the Validation set results
prob_pred = predict(classifier, type = 'response', newdata = test)
y_pred = ifelse(prob_pred > 0.5, 1, 0)

# Checking the prediction accuracy
table(test$Survived, y_pred > 0.5) # Confusion matrix
```
```{r}
error <- mean(test$Survived != y_pred) # Misclassification error
paste('Accuracy',round(1-error,4))
```
```{r}
# Use the predictions to build a ROC curve to assess the performance of our model
fitpred = prediction(prob_pred, test$Survived)
fitperf = performance(fitpred,"tpr","fpr")
plot(fitperf,col="green",lwd=2,main="ROC Curve")
abline(a=0,b=1,lwd=2,lty=2,col="gray")
```
The ROC (Receiver Operating Characteristics) curve is a graphical representation of the performnace of the classifier and it shows the performance of our model rises well above the diagonal line. This indicates that our logistic regression model performs better than just a random guess. The logistic regression model delivers a whooping 0.8539 accuracy interms of predicting the survival.

## Support Vector Machines
```{r}
# Checking the variance of numeric features
paste('Age variance: ', var(train$Age),
      ', SibSp variance: ', var(train$SibSp),
      ', Parch variance: ', var(train$Parch),
      ', Fare variance: ', var(train$Fare))
```
The variances of **Age** and **Fare** seem very high. Let’s do feature scaling to standardize these features so that all features are on the same scale and no feature is dominated by the other.
```{r}
# Feature Scaling - use scale() to standardize the feature columns
standardized.train = cbind(select(train, Survived, Pclass, Sex, SibSp, Parch, Embarked, Title, FamilySize), Age = scale(train$Age), Fare = scale(train$Fare))
paste('Age variance: ',var(standardized.train$Age),', Fare variance: ',var(standardized.train$Fare))
```
```{r}
standardized.test = cbind(select(test, Survived, Pclass, Sex, SibSp, Parch, Embarked, Title, FamilySize), Age = scale(test$Age), Fare = scale(test$Fare))
paste('Age variance: ',var(standardized.test$Age),', Fare variance: ',var(standardized.test$Fare))
```

Now that we have done the feature scaling, we can fit SVM to predict survival. First I fit a linear SVM.
```{r}
# Fitting Linear SVM to the Training set
classifier <- svm(Survived ~ .,
                  data = standardized.train,
                  type = 'C-classification',
                  kernel = 'linear')
# Predicting the Validation set results
y_pred = predict(classifier, newdata = standardized.test[,-which(names(standardized.test)=="Survived")])

# Checking the prediction accuracy
table(test$Survived, y_pred) # Confusion matrix
```
```{r}
error <- mean(test$Survived != y_pred) # Misclassification error
paste('Accuracy',round(1-error,4))
```
 Let’s fit a non-linear radial kernel and see whether the accuracy will be improved.
```{r}
# Fitting Non-linear SVM to the Training set
classifier = svm(Survived ~ .,
                 data = standardized.train,
                 type = 'C-classification',
                 kernel = 'radial')

# Predicting the Validation set results
y_pred = predict(classifier, newdata = standardized.test[,-which(names(standardized.test)=="Survived")])

# Checking the prediction accuracy
table(test$Survived, y_pred) # Confusion matrix
```
```{r}
error <- mean(test$Survived != y_pred) # Misclassification error
paste('Accuracy',round(1-error,4))
```
The accuracy has been improved by 1% to 0.8820. Also, note that both linear SVM and non-linear SVM accuracies are higher than the accuracy for logistic regression model.
The best non-linear SVM performance occurs with cost=1 and gamma=0.0625. Now I tune these parameters to attempt to improve our model.
```{r Applying Grid Search}
# Tuning the model
# Applying Grid Search to find the best parameters
tune.results <- tune(svm,
                     Survived ~ .,
                     data = standardized.train,
                     kernel='radial',
                     ranges=list(cost=2^(-2:2), gamma=2^(-6:-2)))
summary(tune.results)
```
```{r}
# The best non-linear SVM performance occurs with cost=4 and gamma=0.125

# Fitting Non-linear SVM to the Training set
classifier = svm(Survived ~ .,
                 data = standardized.train,
                 type = 'C-classification',
                 kernel = 'radial',
                 cost = 4,
                 gamma = 0.125)

# Predicting the Validation set results
y_pred = predict(classifier, newdata = standardized.test[,-which(names(standardized.test)=="Survived")])

# Checking the prediction accuracy
table(test$Survived, y_pred) # Confusion matrix
```
```{r}
error <- mean(test$Survived != y_pred) # Misclassification error
paste('Accuracy',round(1-error,4))
```
The accuracy went down to 0.8315. We were not able to improve our model. Infact, the best non-linear SVM was already a good model with accuracy 0.8820. I retain that model as my best non-linear SVM model with cost=1 and gamma=0.0625.

## Decision Tree
The Decision Tree does not require feature scaling. Let’s fit a decision tree model to our training data.
```{r}
# Fitting Decision Tree Classification Model to the Training set
classifier = rpart(Survived ~ ., data = train, method = 'class')

# Tree Visualization
rpart.plot(classifier, extra=4)
```
The single tree uses five features Title, Pclass, Fare, Age and FamilySize for classification. Let’s see how well our model performs with the data in the validation set.
```{r}
# Predicting the Validation set results
y_pred = predict(classifier, newdata = test[,-which(names(test)=="Survived")], type='class')

# Checking the prediction accuracy
table(test$Survived, y_pred) # Confusion matrix
```
```{r}
error <- mean(test$Survived != y_pred) # Misclassification error
paste('Accuracy',round(1-error,4))
```
Accuracy of a single tree is 0.8371. Overfitting can easily occur in Decision Tree classification. We can idenfity that evaluating the model using k-Fold Cross Validation. Or we might be able to improve the model. Let’s do 10-fold cross validation to find out whether we could improve the model.
```{r}
# Applying k-Fold Cross Validation
set.seed(789)
folds = createMultiFolds(train$Survived, k = 10, times = 5)
control <- trainControl(method = "repeatedcv", index = folds)
classifier_cv <- train(Survived ~ ., data = train, method = "rpart", trControl = control)

# Tree Visualization
rpart.plot(classifier_cv$finalModel, extra=4)
```
```{r}
# Predicting the Validation set results
y_pred = predict(classifier_cv, newdata = test[,-which(names(test)=="Survived")])

# Checking the prediction accuracy
table(test$Survived, y_pred) # Confusion matrix
```
```{r}
error <- mean(test$Survived != y_pred) # Misclassification error
paste('Accuracy',round(1-error,4))
```
We were able to improve the model after 10-fold cross validation. The accuracy has been improved to 0.8427 but note the improved model uses only three features Title, Pclass and Fare for classification.
##  Random Forests
```{r}
# Fitting Random Forest Classification to the Training set
set.seed(432)
classifier = randomForest(Survived ~ ., data = train)

# Choosing the number of trees
plot(classifier)
```
```{r}
# Predicting the Validation set results
y_pred = predict(classifier, newdata = test[,-which(names(test)=="Survived")])

# Checking the prediction accuracy
table(test$Survived, y_pred) # Confusion matrix
```
```{r}
error <- mean(test$Survived != y_pred) # Misclassification error
paste('Accuracy',round(1-error,4))
```
The accuracy is 0.8427 and which is greater than the accuracy of just a single tree 0.8371 (without 10-fold cross validation). Let’s see if 10-fold cross validation can improve our model as it did for the Decision Tree classification.
```{r}
# Applying k-Fold Cross Validation
set.seed(651)
folds = createMultiFolds(train$Survived, k = 10)
control <- trainControl(method = "repeatedcv", index = folds)
classifier_cv <- train(Survived ~ ., data = train, method = "rf", trControl = control)

# Predicting the Validation set results
y_pred = predict(classifier_cv, newdata = test[,-which(names(test)=="Survived")])

# Checking the prediction accuracy
table(test$Survived, y_pred) # Confusion matrix
```
```{r}
error <- mean(test$Survived != y_pred) # Misclassification error
paste('Accuracy',round(1-error,4))
```
Accuracy went down to 0.8034 We were not able to improve the random forest model using 10-fold cross validation.

As mentioned previously in the Decision Tree section, the random Forest classification suffers in terms of interpretability. We are unable to visualize the 500 trees and identify important features of the model. However, we can assess the **Feature Importance** using the Gini index measure. Let’s plot mean Gini index across all trees and identify important features.
```{r}
# Feature Importance
gini = as.data.frame(importance(classifier))
gini = data.frame(Feature = row.names(gini), 
                  MeanGini = round(gini[ ,'MeanDecreaseGini'], 2))
gini = gini[order(-gini[,"MeanGini"]),]

ggplot(gini,aes(reorder(Feature,MeanGini), MeanGini, group=1)) + 
  geom_point(color='red',shape=17,size=2) + 
  geom_line(color='blue',size=1) +
  scale_y_continuous(breaks=seq(0,60,10)) +
  xlab("Feature") + 
  ggtitle("Mean Gini Index of Features") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
The feature Title has the highest mean gini index, hence the highest importance. Fare is also realtively high important and it is followed by Age of the passengers.

## Naive Bayes
 Naive Bayes is based on the assumption that conditional probability of each feature given the class is independent of all the other features. The assumption of independent conditional probabilities means the features are completely independent of each other. This assumption was already checked in the Logistic Regression section and we have found that numeric features are independent to each other, however, the categorical features are not. By assuming the idependence assumption of all the features, let’s fit a naive bayes model to our training data.
```{r}
# Fitting Naive Bayes to the Training set
classifier = naiveBayes(Survived ~ ., data = train)

# Predicting the Validation set results
y_pred = predict(classifier, newdata = test[,-which(names(test)=="Survived")])

# Checking the prediction accuracy
table(test$Survived, y_pred) # Confusion matrix
```
```{r}
error <- mean(test$Survived != y_pred) # Misclassification error
paste('Accuracy',round(1-error,4))
```
Naive Bayes classification performs well for our validation data with an accuracy of 0.8427. Note that the accuracy is identical to the Random Forest classification accuracy (without 10-fold cross validation).

# Results
```{r}
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_original)

# Save the results
results <- data.frame(PassengerID = titanic[892:1309,"PassengerId"], Survived = y_pred)

# Write the results to a csv file
write.csv(results, file = 'PredictingTitanicSurvival.csv', row.names = FALSE, quote=FALSE)
```

 
